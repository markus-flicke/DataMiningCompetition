{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Just use all features quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20, 8)   \n",
    "plt.rcParams['figure.dpi'] = 72             \n",
    "plt.rcParams['font.size'] = 16               \n",
    "plt.rcParams['text.usetex'] = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'train_imputed_pay_12_extracted_no_profession_children.csv'\n",
    "df_train = pd.read_csv(FILE_NAME)\n",
    "df_train = df_train.sample(frac=1)\n",
    "ids = df_train.id\n",
    "df_train = df_train.drop('id', axis = 1)\n",
    "N = 14000\n",
    "def train_test_split(N, df_train=df_train):\n",
    "#     df_train = df_train.sample(frac=1)\n",
    "    x_train = df_train.drop('target', axis = 1).iloc[:N]\n",
    "    y_train = df_train.target.iloc[:N]\n",
    "\n",
    "    x_test = df_train.drop('target', axis = 1).iloc[N:]\n",
    "    y_test = df_train.target.iloc[N:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "x_train, y_train, x_test, y_test = train_test_split(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cheating import real_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) is great 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.7584502304858951\n",
      "MSE: 0.14499953088257145\n",
      "0.7561562373293688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7584502304858951, 0.14499953088257145)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_xgboost(rounds = 10, verbose = True, n_jobs = 8):\n",
    "    dtrain = xgb.DMatrix(x_train.values, label=y_train.values)\n",
    "    dtest = xgb.DMatrix(x_test.values, label=y_test.values)\n",
    "\n",
    "    param = {'max_depth': 2, \n",
    "             'eta': 1, \n",
    "             'objective': 'binary:logistic',\n",
    "             'nthread': n_jobs,\n",
    "             'eval_metric': 'auc',\n",
    "            }\n",
    "\n",
    "\n",
    "    evallist = [(dtrain, 'train')] # (dtest, 'eval'), \n",
    "    num_round = rounds\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n",
    "    preds = bst.predict(dtest)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'roc_auc: {roc_auc_score(y_test, preds)}')\n",
    "        print(f'MSE: {mean_squared_error(y_test, preds)}')\n",
    "        \n",
    "    print(real_auc(preds, ids[N:]))\n",
    "    return roc_auc_score(y_test, preds), mean_squared_error(y_test, preds)\n",
    "eval_xgboost(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "auc_scores = []\n",
    "mse_scores = []\n",
    "x = list(range(1,20)) + list(range(20, 101, 10))\n",
    "for i in x:\n",
    "    auc_sum = 0\n",
    "    mse_sum = 0\n",
    "    for j in range(5):\n",
    "        x_train, y_train, x_test, y_test = train_test_split()\n",
    "        auc, mse = eval_xgboost(i, verbose = False)\n",
    "        auc_sum += auc\n",
    "        mse_sum += mse\n",
    "    auc_scores.append(auc_sum/5)\n",
    "    mse_scores.append(mse_sum/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "ax2 = ax1.twinx()\n",
    "ax1.scatter(x, auc_scores, marker = 'x', label='auc score', s = 100)\n",
    "ax2.plot(x, mse_scores, marker = '.', label='mse score', color = 'green')\n",
    "fig.legend()\n",
    "plt.xlabel('Rounds of XGboost')\n",
    "ax1.set_ylabel('AUC score')\n",
    "plt.title(FILE_NAME)\n",
    "ax1.grid()\n",
    "ax1.hlines(max(auc_scores), 1, 100, color = 'red')\n",
    "print(f'max auc at {x[np.argmax(auc_scores)]} rounds')\n",
    "plt.savefig(FILE_NAME+'_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest is consistent 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(1000, criterion='entropy', n_jobs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train)\n",
    "preds = model.predict(x_test)\n",
    "print(f'roc_auc: {roc_auc_score(y_test, preds)}')\n",
    "print(f'accuracy: {accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric':'auc',\n",
    "          'learning_rate': 0.05,\n",
    "          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n",
    "          'max_depth': 4,  # -1 means no limit\n",
    "          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "          'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "          'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "          'nthread': 8,\n",
    "          'verbose': 0,\n",
    "          'scale_pos_weight':50, # because training data is sightly unbalanced \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(x_train, \n",
    "                     label=y_train)\n",
    "\n",
    "dvalid = lgb.Dataset(x_test, \n",
    "                     label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_results = {}\n",
    "MAX_ROUNDS = 1000 #lgb iterations\n",
    "EARLY_STOP = 50 #lgb early stop \n",
    "OPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\n",
    "VERBOSE_EVAL = 50 #Print out metric result\n",
    "\n",
    "\n",
    "\n",
    "model = lgb.train(params, \n",
    "                  dtrain, \n",
    "                  valid_sets=[dtrain, dvalid], \n",
    "                  valid_names=['train','valid'], \n",
    "                  evals_result=evals_results, \n",
    "                  num_boost_round=MAX_ROUNDS,\n",
    "                  early_stopping_rounds=EARLY_STOP,\n",
    "                  verbose_eval=VERBOSE_EVAL, \n",
    "                  feval=None)\n",
    "\n",
    "del dvalid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN is ok at 55%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "x = range(2,10)\n",
    "for k in x:\n",
    "    model = KNeighborsClassifier(k)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    scores.append(roc_auc_score(y_test, preds))\n",
    "plt.plot(x, scores)\n",
    "print(max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron is ok-ish 54%\n",
    "High variance in outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train)\n",
    "preds = model.predict(x_test)\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression is shit 50%\n",
    "Few predictions are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)\n",
    "preds = model.predict(x_test)\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC is completely useless 50%\n",
    "All predictions are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SVC()\n",
    "model.fit(x_train, y_train)\n",
    "preds = model.predict(x_test)\n",
    "roc_auc_score(y_test, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
